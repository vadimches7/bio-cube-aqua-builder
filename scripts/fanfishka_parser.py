#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –∞–∫–≤–∞—Ä–∏—É–º–Ω—ã—Ö —Ä—ã–±–æ–∫ —Å —Å–∞–π—Ç–∞ fanfishka.ru
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import re
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = "https://fanfishka.ru"
START_URL = "https://fanfishka.ru/akvariumnye-stati/akvariumnye_rybki/page/1/"
DELAY_BETWEEN_REQUESTS = 1  # —Å–µ–∫—É–Ω–¥—ã
OUTPUT_FILE = "fish_catalog.json"

# User-Agent –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


class FanFishkaParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ä—ã–± —Å fanfishka.ru"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.fish_links = []
        self.fish_data = []
        self.fish_id_counter = 1
    
    def get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return BeautifulSoup(response.text, 'html.parser')
            except requests.RequestException as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(2)
                else:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}")
                    return None
        return None
    
    def find_last_page(self) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–æ–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
        logger.info("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞...")
        soup = self.get_page(START_URL)
        if not soup:
            return 1
        
        last_page = 1
        page_numbers = []
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        pagination_selectors = [
            '.pagination a',
            '.page-numbers a',
            '.pager a',
            '.pagination-nav a',
            'nav.pagination a',
            '.wp-pagenavi a',
            '.pagination li a',
            '.page-nav a'
        ]
        
        for selector in pagination_selectors:
            links = soup.select(selector)
            if links:
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ URL
                    if '/page/' in href:
                        match = re.search(r'/page/(\d+)/', href)
                        if match:
                            page_numbers.append(int(match.group(1)))
                    # –ò–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
                    elif text.isdigit():
                        try:
                            page_numbers.append(int(text))
                        except ValueError:
                            pass
                
                if page_numbers:
                    last_page = max(page_numbers)
                    logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä {selector}: {last_page}")
                    break
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å /page/
        if last_page == 1:
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                if '/page/' in href:
                    match = re.search(r'/page/(\d+)/', href)
                    if match:
                        page_numbers.append(int(match.group(1)))
            
            if page_numbers:
                last_page = max(page_numbers)
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –ø–æ–∏—Å–∫–æ–º: {last_page}")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 3: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ –∏—Å–∫–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if last_page == 1:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Ä—É—á–Ω—É—é...")
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            for test_page in [2, 3, 5, 10, 20, 50]:
                test_url = f"https://fanfishka.ru/akvariumnye-stati/akvariumnye_rybki/page/{test_page}/"
                test_soup = self.get_page(test_url)
                if test_soup:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    content = test_soup.get_text()
                    if len(content) > 1000:  # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        last_page = test_page
                        time.sleep(0.5)
                    else:
                        break
                else:
                    break
            
            if last_page > 1:
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–≤–µ—Ä–∫–∏: {last_page}")
        
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {last_page}")
        return max(last_page, 1)
    
    def collect_fish_links_from_page(self, page_url: str) -> List[str]:
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –æ —Ä—ã–±–∞—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
        soup = self.get_page(page_url)
        if not soup:
            return []
        
        links = []
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        article_selectors = [
            '.post-box a',
            '.article-item a',
            '.post-card a',
            '.entry-title a',
            '.post-title a',
            'article a',
            '.post a',
            '.fish-card a',
            '.item a',
            '.card a',
            'h2 a',
            'h3 a',
            'h4 a'
        ]
        
        found_with_selector = False
        for selector in article_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    href = element.get('href', '')
                    if href:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL —Å—Ç–∞—Ç–µ–π
                        if any(pattern in href for pattern in [
                            '/akvariumnye-stati/akvariumnye_rybki/',
                            '/akvariumnye-stati/',
                            '/rybki/',
                            '/fish/'
                        ]) and '/page/' not in href:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                            full_url = urljoin(BASE_URL, href)
                            if full_url not in links and full_url not in self.fish_links:
                                links.append(full_url)
                
                if links:
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_url} (—Å–µ–ª–µ–∫—Ç–æ—Ä: {selector})")
                    found_with_selector = True
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if not found_with_selector:
            logger.warning(f"–°–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                if href:
                    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –Ω–∞ —Å—Ç–∞—Ç—å–∏ –æ —Ä—ã–±–∞—Ö
                    if any(pattern in href for pattern in [
                        '/akvariumnye-stati/akvariumnye_rybki/',
                        '/akvariumnye-stati/'
                    ]) and '/page/' not in href and href not in ['#', '']:
                        full_url = urljoin(BASE_URL, href)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∫–∞—Ç–∞–ª–æ–≥–∞
                        if full_url != page_url and full_url not in links and full_url not in self.fish_links:
                            links.append(full_url)
            
            if links:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –ø–æ–∏—Å–∫–æ–º")
            else:
                # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
                # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                sample_links = soup.find_all('a', href=True, limit=10)
                logger.debug(f"–ü—Ä–∏–º–µ—Ä—ã —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ:")
                for sample in sample_links[:5]:
                    logger.debug(f"  - {sample.get('href', '')}")
        
        return links
    
    def extract_latin_name(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –ª–∞—Ç–∏–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª–∞—Ç–∏–Ω—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
        patterns = [
            r'\(([A-Z][a-z]+(?:\s+[a-z]+)+)\)',  # (Paracheirodon innesi)
            r'([A-Z][a-z]+\s+[a-z]+)',  # Paracheirodon innesi
            r'–õ–∞—Ç–∏–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ[:\s]+([A-Z][a-z]+(?:\s+[a-z]+)+)',
            r'–ù–∞—É—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ[:\s]+([A-Z][a-z]+(?:\s+[a-z]+)+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return ""
    
    def extract_water_params(self, text: str) -> Dict[str, Optional[float]]:
        """–ò–∑–≤–ª–µ—á—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        params = {
            'ph_min': None,
            'ph_max': None,
            'temp_min': None,
            'temp_max': None
        }
        
        # –ü–æ–∏—Å–∫ pH
        ph_patterns = [
            r'pH[:\s]+([\d,\.]+)[\s\-‚Äì‚Äî]+([\d,\.]+)',
            r'pH[:\s]+([\d,\.]+)',
            r'–∫–∏—Å–ª–æ—Ç–Ω–æ—Å—Ç—å[:\s]+([\d,\.]+)[\s\-‚Äì‚Äî]+([\d,\.]+)',
        ]
        
        for pattern in ph_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    if len(match.groups()) == 2:
                        params['ph_min'] = float(match.group(1).replace(',', '.'))
                        params['ph_max'] = float(match.group(2).replace(',', '.'))
                    else:
                        ph_value = float(match.group(1).replace(',', '.'))
                        params['ph_min'] = ph_value - 0.5
                        params['ph_max'] = ph_value + 0.5
                    break
                except ValueError:
                    continue
        
        # –ü–æ–∏—Å–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
        temp_patterns = [
            r'—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä[–∞—ã][:\s]+([\d,\.]+)[\s\-‚Äì‚Äî¬∞]+([\d,\.]+)',
            r'(\d+)[\s\-‚Äì‚Äî¬∞]+(\d+)\s*¬∞[–°C]',
            r'(\d+)[\s\-‚Äì‚Äî]+(\d+)\s*–≥—Ä–∞–¥—É—Å',
        ]
        
        for pattern in temp_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    params['temp_min'] = float(match.group(1).replace(',', '.'))
                    params['temp_max'] = float(match.group(2).replace(',', '.'))
                    break
                except ValueError:
                    continue
        
        return params
    
    def extract_min_volume(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º –∞–∫–≤–∞—Ä–∏—É–º–∞"""
        patterns = [
            r'–º–∏–Ω–∏–º–∞–ª—å–Ω[—ã–π]+[–π\s]+–æ–±—ä–µ–º[:\s]+(\d+)',
            r'–æ—Ç\s+(\d+)\s+–ª–∏—Ç—Ä',
            r'–º–∏–Ω–∏–º—É–º[:\s]+(\d+)\s+–ª',
            r'–æ–±—ä–µ–º[:\s]+(\d+)\s+–ª',
            r'–∞–∫–≤–∞—Ä–∏—É–º[:\s]+(\d+)\s+–ª',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue
        
        return None
    
    def extract_size(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á—å —Ä–∞–∑–º–µ—Ä —Ä—ã–±—ã –≤ —Å–º"""
        patterns = [
            r'—Ä–∞–∑–º–µ—Ä[:\s]+–¥–æ\s+(\d+[,\.]?\d*)\s*—Å–º',
            r'–¥–ª–∏–Ω–∞[:\s]+(\d+[,\.]?\d*)\s*—Å–º',
            r'(\d+[,\.]?\d*)\s*—Å–º\s+–≤\s+–¥–ª–∏–Ω—É',
            r'–¥–æ\s+(\d+[,\.]?\d*)\s*—Å–º',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    size_str = match.group(1).replace(',', '.')
                    return float(size_str)
                except ValueError:
                    continue
        
        return None
    
    def extract_temperament(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞–º–µ–Ω—Ç —Ä—ã–±—ã"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['–º–∏—Ä–Ω', '—Å–ø–æ–∫–æ–π–Ω', 'peaceful', '–¥—Ä—É–∂–µ–ª—é–±–Ω']):
            return "–ú–∏—Ä–Ω—ã–π"
        elif any(word in text_lower for word in ['–∞–≥—Ä–µ—Å—Å–∏–≤–Ω', '—Ö–∏—â–Ω', 'aggressive', 'predator']):
            return "–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π"
        elif any(word in text_lower for word in ['—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∞–ª—å–Ω', '–ø–æ–ª—É–∞–≥—Ä–µ—Å—Å–∏–≤–Ω', 'semi-aggressive']):
            return "–ü–æ–ª—É–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π"
        
        return "–ú–∏—Ä–Ω—ã–π"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def extract_min_group_size(self, text: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å—Ç–∞–∏"""
        patterns = [
            r'—Å—Ç–∞–π–Ω[–∞—è]+[–π\s]+(\d+)',
            r'–≥—Ä—É–ø–ø[–∞—ã][:\s]+–æ—Ç\s+(\d+)',
            r'–º–∏–Ω–∏–º—É–º[:\s]+(\d+)\s+–æ—Å–æ–±',
            r'—Å–æ–¥–µ—Ä–∂–∞—Ç—å[:\s]+–æ—Ç\s+(\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue
        
        # –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è "—Å—Ç–∞–π–Ω–∞—è", –Ω–æ –Ω–µ—Ç —á–∏—Å–ª–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 6
        if any(word in text.lower() for word in ['—Å—Ç–∞–π–Ω', '–≥—Ä—É–ø–ø', 'school']):
            return 6
        
        return 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–¥–∏–Ω–æ—á–Ω–∞—è
    
    def determine_difficulty(self, text: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (1-3)"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['–ª–µ–≥–∫', '–ø—Ä–æ—Å—Ç–æ–π', '–Ω–µ–ø—Ä–∏—Ö–æ—Ç–ª–∏–≤', '–Ω–∞—á–∏–Ω–∞—é—â', 'easy', 'beginner']):
            return 1
        elif any(word in text_lower for word in ['—Å–ª–æ–∂–Ω', '—Ç—Ä—É–¥–Ω', '—Ç—Ä–µ–±–æ–≤–∞—Ç–µ–ª—å–Ω', 'advanced', 'expert']):
            return 3
        else:
            return 2  # –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    
    def determine_fish_type(self, text: str, url: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ä—ã–±—ã (freshwater/marine)"""
        text_lower = text.lower()
        url_lower = url.lower()
        
        if any(word in text_lower or word in url_lower for word in ['–º–æ—Ä—Å–∫', 'marine', 'saltwater', 'reef']):
            return "marine"
        
        return "freshwater"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ—Å–Ω–æ–≤–æ–¥–Ω–∞—è
    
    def parse_fish_article(self, url: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –æ —Ä—ã–±–µ"""
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏: {url}")
        soup = self.get_page(url)
        if not soup:
            return None
        
        fish_data = {
            'id': self.fish_id_counter,
            'name_ru': '',
            'name_lat': '',
            'type': 'freshwater',
            'family_group': '',
            'size_cm': 0,
            'min_tank_liters': 0,
            'bio_load_points': 2,
            'temperament': '–ú–∏—Ä–Ω—ã–π',
            'min_group_size': 1,
            'difficulty': 2,
            'water_params': {
                'ph_min': None,
                'ph_max': None,
                'temp_min': None,
                'temp_max': None
            },
            'incompatible_tags': [],
            'description_short': '',
            'features_list': [],
            'image_url': '',  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            'article_url': url  # –°–æ—Ö—Ä–∞–Ω—è–µ–º URL —Å—Ç–∞—Ç—å–∏ –¥–ª—è –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–Ω–≥–∞
        }
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (name_ru)
        title_selectors = ['h1', '.entry-title', '.post-title', '.article-title', 'title']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text:
                    fish_data['name_ru'] = title_text
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∞—Ç–∏–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    fish_data['name_lat'] = self.extract_latin_name(title_text)
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ª–∞—Ç–∏–Ω—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        if not fish_data['name_lat']:
            article_text = soup.get_text()
            fish_data['name_lat'] = self.extract_latin_name(article_text)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        image_url = None
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ò—â–µ–º –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_containers = [
            '.entry-content',
            '.post-content',
            '.article-content',
            '.content',
            'article',
            '.post-body',
            '.single-post',
            'main article'
        ]
        
        for container_selector in content_containers:
            container = soup.select_one(container_selector)
            if container:
                images = container.find_all('img')
                for img in images:
                    img_src = (img.get('src') or 
                              img.get('data-src') or 
                              img.get('data-lazy-src') or
                              img.get('data-original') or
                              img.get('data-url'))
                    
                    if img_src:
                        img_src = urljoin(BASE_URL, img_src)
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if 'sovmestimost_akvaryb.png' in img_src.lower():
                            continue
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏ –∏ –ª–æ–≥–æ—Ç–∏–ø—ã
                        skip_patterns = ['logo', 'icon', 'avatar', 'banner', 'thumb', 'thumbnail', 'wp-', 'emoji']
                        if any(skip in img_src.lower() for skip in skip_patterns):
                            continue
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
                        width = img.get('width') or img.get('data-width') or '0'
                        try:
                            w = int(str(width).replace('px', ''))
                            if w > 200:  # –¢–æ–ª—å–∫–æ –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                image_url = img_src
                                break
                        except:
                            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ —É–∫–∞–∑–∞–Ω, –±–µ—Ä–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            image_url = img_src
                            break
                
                if image_url:
                    break
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –±–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ
        if not image_url:
            all_images = soup.find_all('img')
            candidate_images = []
            
            for img in all_images:
                img_src = (img.get('src') or 
                          img.get('data-src') or 
                          img.get('data-lazy-src') or
                          img.get('data-original'))
                
                if img_src:
                    img_src = urljoin(BASE_URL, img_src)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
                    if 'sovmestimost' in img_src.lower():
                        continue
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏
                    skip_patterns = ['logo', 'icon', 'avatar', 'banner', 'thumb', 'wp-admin']
                    if any(skip in img_src.lower() for skip in skip_patterns):
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä
                    width = img.get('width') or img.get('data-width') or '0'
                    height = img.get('height') or img.get('data-height') or '0'
                    
                    try:
                        w = int(str(width).replace('px', '')) if width else 0
                        h = int(str(height).replace('px', '')) if height else 0
                        size = w * h if w > 0 and h > 0 else 1000
                        candidate_images.append((size, img_src))
                    except:
                        candidate_images.append((1000, img_src))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –±–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ
            if candidate_images:
                candidate_images.sort(reverse=True, key=lambda x: x[0])
                image_url = candidate_images[0][1]
        
        if image_url:
            fish_data['image_url'] = image_url
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
        content_selectors = [
            '.entry-content',
            '.post-content',
            '.article-content',
            '.content',
            'article',
            '.post-body'
        ]
        
        article_text = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                article_text = content_elem.get_text()
                break
        
        if not article_text:
            article_text = soup.get_text()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è (description_short)
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö –∞–±–∑–∞—Ü–µ–≤ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
        paragraphs = soup.select('p')
        description_parts = []
        for para in paragraphs[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –∞–±–∑–∞—Ü–µ–≤
            text = para.get_text(strip=True)
            if len(text) > 30:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
                description_parts.append(text)
        
        if description_parts:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–±–∑–∞—Ü—ã –≤ –æ–¥–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ
            full_description = ' '.join(description_parts)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤
            fish_data['description_short'] = full_description[:1000]
        elif article_text:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∞–±–∑–∞—Ü—ã, –±–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
            fish_data['description_short'] = article_text[:1000].strip()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–æ–¥—ã
        water_params = self.extract_water_params(article_text)
        fish_data['water_params'].update(water_params)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–º–∞
        min_volume = self.extract_min_volume(article_text)
        if min_volume:
            fish_data['min_tank_liters'] = min_volume
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        size = self.extract_size(article_text)
        if size:
            fish_data['size_cm'] = int(size)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞–º–µ–Ω—Ç–∞
        fish_data['temperament'] = self.extract_temperament(article_text)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å—Ç–∞–∏
        fish_data['min_group_size'] = self.extract_min_group_size(article_text)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        fish_data['difficulty'] = self.determine_difficulty(article_text)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ (–ø—Ä–µ—Å–Ω–æ–≤–æ–¥–Ω–∞—è/–º–æ—Ä—Å–∫–∞—è)
        fish_data['type'] = self.determine_fish_type(article_text, url)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–µ–π—Å—Ç–≤–∞ (family_group)
        family_patterns = [
            r'—Å–µ–º–µ–π—Å—Ç–≤[–æ–∞][:\s]+([–ê-–Ø–∞-—è\s]+)',
            r'–æ—Ç—Ä—è–¥[:\s]+([–ê-–Ø–∞-—è\s]+)',
        ]
        
        for pattern in family_patterns:
            match = re.search(pattern, article_text, re.IGNORECASE)
            if match:
                fish_data['family_group'] = match.group(1).strip()
                break
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π
        features = []
        if fish_data['water_params']['temp_min']:
            features.append(f"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {fish_data['water_params']['temp_min']}-{fish_data['water_params']['temp_max']}¬∞C")
        if fish_data['water_params']['ph_min']:
            features.append(f"pH: {fish_data['water_params']['ph_min']}-{fish_data['water_params']['ph_max']}")
        if fish_data['min_tank_liters']:
            features.append(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º: {fish_data['min_tank_liters']} –ª")
        if fish_data['temperament']:
            features.append(f"–¢–µ–º–ø–µ—Ä–∞–º–µ–Ω—Ç: {fish_data['temperament']}")
        
        fish_data['features_list'] = features
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º image_url (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω—É–∂–Ω—ã —Ñ–æ—Ç–æ)
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ —Å—Ç–∞—Ç—å–µ
        if not fish_data.get('image_url'):
            all_images = soup.find_all('img')
            for img in all_images:
                img_src = img.get('src') or img.get('data-src')
                if img_src and not any(skip in img_src.lower() for skip in ['logo', 'icon', 'avatar', 'banner']):
                    fish_data['image_url'] = urljoin(BASE_URL, img_src)
                    break
        
        self.fish_id_counter += 1
        return fish_data
    
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        logger.info("–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ fanfishka.ru")
        
        # –®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        last_page = self.find_last_page()
        
        # –®–∞–≥ 2: –°–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        logger.info(f"–°–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü 1-{last_page}...")
        for page_num in range(1, last_page + 1):
            page_url = f"https://fanfishka.ru/akvariumnye-stati/akvariumnye_rybki/page/{page_num}/"
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/{last_page}")
            
            links = self.collect_fish_links_from_page(page_url)
            self.fish_links.extend(links)
            
            time.sleep(DELAY_BETWEEN_REQUESTS)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        self.fish_links = list(set(self.fish_links))
        logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(self.fish_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏")
        
        # –®–∞–≥ 3: –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏
        logger.info("–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç–µ–π...")
        for i, link in enumerate(self.fish_links, 1):
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {i}/{len(self.fish_links)}")
            
            fish_data = self.parse_fish_article(link)
            if fish_data:
                self.fish_data.append(fish_data)
                has_photo = "‚úÖ" if fish_data.get('image_url') else "‚ùå"
                logger.info(f"‚úì –°–æ–±—Ä–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ: {fish_data['name_ru']} {has_photo} —Ñ–æ—Ç–æ")
            else:
                logger.warning(f"‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ {link}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥—ã–µ 10 —Å—Ç–∞—Ç–µ–π
            if i % 10 == 0:
                with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                    json.dump(self.fish_data, f, ensure_ascii=False, indent=2)
                logger.info(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(self.fish_data)} –∑–∞–ø–∏—Å–µ–π")
            
            time.sleep(DELAY_BETWEEN_REQUESTS)
        
        # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(self.fish_data)} –∑–∞–ø–∏—Å–µ–π –≤ {OUTPUT_FILE}...")
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.fish_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úì –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_FILE}")
        logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.fish_data)} —Ä—ã–±")


if __name__ == "__main__":
    parser = FanFishkaParser()
    try:
        parser.run()
    except KeyboardInterrupt:
        logger.info("\n–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        if parser.fish_data:
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({len(parser.fish_data)} –∑–∞–ø–∏—Å–µ–π)...")
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(parser.fish_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)

