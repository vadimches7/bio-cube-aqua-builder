#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–ª–∞–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä—ã–± —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ö–ê–¢–ê–õ–û–ì–ê
(–≥–¥–µ –≤–∏–¥–Ω—ã –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —Ñ–æ—Ç–æ, –∫–∞–∫ –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–µ)
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import re
from urllib.parse import urljoin
from pathlib import Path
from typing import Optional, Dict

BASE_DIR = Path(__file__).parent.parent
CATALOG_PATH = BASE_DIR / 'fish_catalog.json'
OUTPUT_PATH = BASE_DIR / 'fish_catalog.json'

BASE_URL = "https://fanfishka.ru"
CATALOG_BASE_URL = "https://fanfishka.ru/akvariumnye-stati/akvariumnye_rybki/page/"
DELAY_BETWEEN_REQUESTS = 1

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_page(url: str, retries: int = 3) -> Optional[BeautifulSoup]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return BeautifulSoup(response.text, 'html.parser')
        except requests.RequestException as e:
            if attempt < retries - 1:
                time.sleep(2)
            else:
                return None
    return None

def extract_fish_images_from_catalog_page(soup: BeautifulSoup, page_url: str) -> Dict[str, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä—ã–± —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: {–Ω–∞–∑–≤–∞–Ω–∏–µ_—Ä—ã–±—ã: url_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è}
    """
    fish_images = {}
    
    # –ò—â–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ä—ã–± –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–∞–ª–æ–≥–∞
    # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫
    card_selectors = [
        '.post-box',
        '.article-item',
        '.post-card',
        '.fish-card',
        '.item',
        'article',
        '.entry'
    ]
    
    cards = []
    for selector in card_selectors:
        found_cards = soup.select(selector)
        if found_cards:
            cards = found_cards
            print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}': {len(cards)}")
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –∏—â–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
    if not cards:
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –æ —Ä—ã–±–∞—Ö –∏ –∏—Ö —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        fish_links = soup.find_all('a', href=re.compile(r'/akvariumnye-stati/akvariumnye_rybki/'))
        for link in fish_links:
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫—É (—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç)
            card = link.find_parent(['article', 'div', 'li'])
            if card and card not in cards:
                cards.append(card)
    
    print(f"   –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    for card in cards:
        # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä—ã–±—ã (–æ–±—ã—á–Ω–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏–ª–∏ —Å—Å—ã–ª–∫–µ)
        title_elem = card.select_one('h2, h3, h4, .title, .entry-title, .post-title, a')
        if not title_elem:
            continue
        
        fish_name = title_elem.get_text(strip=True)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-—Ä—ã–±—ã
        if any(kw in fish_name.lower() for kw in ['—Ä–∞—Å—Ç–µ–Ω–∏', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω', '—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö', '–∫–∞—Ç–∞–ª–æ–≥']):
            continue
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ
        img = card.select_one('img')
        if img:
            img_src = (img.get('src') or 
                      img.get('data-src') or 
                      img.get('data-lazy-src') or
                      img.get('data-original') or
                      img.get('data-url'))
            
            if img_src:
                img_src = urljoin(BASE_URL, img_src)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∏ –±–∞–Ω–Ω–µ—Ä—ã
                skip_patterns = [
                    'sovmestimost_akvaryb.png',
                    '–±–∞–Ω–Ω–µ—Ä', 'banner', 'navigator',
                    'logo', 'icon', 'avatar', 'thumb', 'widget'
                ]
                
                if not any(skip in img_src.lower() for skip in skip_patterns):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
                    width = img.get('width') or img.get('data-width') or '0'
                    try:
                        w = int(str(width).replace('px', ''))
                        if w > 150:  # –¢–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            fish_images[fish_name] = img_src
                            print(f"      ‚úì {fish_name[:30]}: {img_src[:50]}...")
                    except:
                        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ —É–∫–∞–∑–∞–Ω, –Ω–æ —ç—Ç–æ –Ω–µ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ - –±–µ—Ä–µ–º
                        fish_images[fish_name] = img_src
                        print(f"      ‚úì {fish_name[:30]}: {img_src[:50]}...")
    
    return fish_images

def normalize_name(name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    return re.sub(r'[^\w\s]', '', name.lower().strip())

def main():
    print("=" * 60)
    print("–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ì–õ–ê–í–ù–´–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô –°–û –°–¢–†–ê–ù–ò–¶ –ö–ê–¢–ê–õ–û–ì–ê")
    print("=" * 60)
    print()
    
    # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–∞—Ç–∞–ª–æ–≥
    print("üìñ –ß—Ç–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞...")
    with open(CATALOG_PATH, 'r', encoding='utf-8') as f:
        catalog_data = json.load(f)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(catalog_data)} –∑–∞–ø–∏—Å–µ–π")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ –æ —Ä—ã–±–∞—Ö
    fish_articles = [
        item for item in catalog_data 
        if (item.get('size_cm', 0) > 0 or item.get('min_tank_liters', 0) > 0) and
           not any(kw in item.get('name_ru', '').lower() for kw in ['—Ä–∞—Å—Ç–µ–Ω–∏', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω', '—Å–ø–∏—Å–æ–∫'])
    ]
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(fish_articles)} —Å—Ç–∞—Ç–µ–π –æ —Ä—ã–±–∞—Ö")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é
    catalog_dict = {item['id']: item for item in catalog_data}
    fish_name_map = {normalize_name(item['name_ru']): item['id'] for item in fish_articles}
    
    print(f"\nüîÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    print()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    first_page = get_page(f"{CATALOG_BASE_URL}1/")
    if not first_page:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞")
        return
    
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    last_page = 1
    pagination_links = first_page.find_all('a', href=re.compile(r'/page/\d+/'))
    for link in pagination_links:
        href = link.get('href', '')
        match = re.search(r'/page/(\d+)/', href)
        if match:
            last_page = max(last_page, int(match.group(1)))
    
    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞: {last_page}")
    print()
    
    total_updated = 0
    
    # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞
    for page_num in range(1, min(last_page + 1, 40)):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 40 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
        print(f"[–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{last_page}] –ü–∞—Ä—Å–∏–Ω–≥...")
        
        page_url = f"{CATALOG_BASE_URL}{page_num}/"
        soup = get_page(page_url)
        
        if not soup:
            print(f"   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        fish_images = extract_fish_images_from_catalog_page(soup, page_url)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–∞–ª–æ–≥
        for fish_name, image_url in fish_images.items():
            normalized = normalize_name(fish_name)
            if normalized in fish_name_map:
                fish_id = fish_name_map[normalized]
                if fish_id in catalog_dict:
                    old_image = catalog_dict[fish_id].get('image_url', '')
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç–∞—Ä–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–ª–∏ –±–∞–Ω–Ω–µ—Ä
                    if ('sovmestimost' in old_image.lower() or 
                        '–±–∞–Ω–Ω–µ—Ä' in old_image.lower() or 
                        'banner' in old_image.lower() or
                        not old_image):
                        catalog_dict[fish_id]['image_url'] = image_url
                        total_updated += 1
                        print(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {catalog_dict[fish_id]['name_ru'][:30]}")
        
        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
        if page_num % 10 == 0:
            with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
                json.dump(list(catalog_dict.values()), f, ensure_ascii=False, indent=2)
            print(f"\nüíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ({page_num} —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ)\n")
        
        time.sleep(DELAY_BETWEEN_REQUESTS)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(list(catalog_dict.values()), f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 60)
    print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_updated}")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_PATH}")
    print()
    print("‚ú® –ì–æ—Ç–æ–≤–æ!")

if __name__ == "__main__":
    main()

