#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —É–∂–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ —Ä—ã–±–∞—Ö
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import re
from urllib.parse import urljoin
from pathlib import Path
from typing import Optional

BASE_DIR = Path(__file__).parent.parent
CATALOG_PATH = BASE_DIR / 'fish_catalog.json'
OUTPUT_PATH = BASE_DIR / 'fish_catalog_updated.json'

BASE_URL = "https://fanfishka.ru"
DELAY_BETWEEN_REQUESTS = 1

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_page(url: str, retries: int = 3) -> Optional[BeautifulSoup]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return BeautifulSoup(response.text, 'html.parser')
        except requests.RequestException as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(2)
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}")
                return None
    return None

def extract_image_from_page(soup: BeautifulSoup, article_url: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä—ã–±—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    image_url = None
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ò—â–µ–º –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_containers = [
        '.entry-content',
        '.post-content',
        '.article-content',
        '.content',
        'article',
        '.post-body',
        '.single-post',
        'main article',
        '.article-body'
    ]
    
    for container_selector in content_containers:
        container = soup.select_one(container_selector)
        if container:
            images = container.find_all('img')
            for img in images:
                img_src = (img.get('src') or 
                          img.get('data-src') or 
                          img.get('data-lazy-src') or
                          img.get('data-original') or
                          img.get('data-url'))
                
                if img_src:
                    img_src = urljoin(BASE_URL, img_src)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if 'sovmestimost_akvaryb.png' in img_src.lower():
                        continue
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏ –∏ –ª–æ–≥–æ—Ç–∏–ø—ã
                    skip_patterns = [
                        'logo', 'icon', 'avatar', 'banner', 'thumb', 
                        'thumbnail', 'wp-', 'emoji', 'button', 'arrow',
                        'social', 'share', 'comment'
                    ]
                    if any(skip in img_src.lower() for skip in skip_patterns):
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
                    width = img.get('width') or img.get('data-width') or '0'
                    try:
                        w = int(str(width).replace('px', ''))
                        if w > 200:  # –¢–æ–ª—å–∫–æ –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            image_url = img_src
                            break
                    except:
                        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ - –±–µ—Ä–µ–º
                        if 'sovmestimost' not in img_src.lower():
                            image_url = img_src
                            break
            
            if image_url:
                break
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –±–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ
    if not image_url:
        all_images = soup.find_all('img')
        candidate_images = []
        
        for img in all_images:
            img_src = (img.get('src') or 
                      img.get('data-src') or 
                      img.get('data-lazy-src') or
                      img.get('data-original'))
            
            if img_src:
                img_src = urljoin(BASE_URL, img_src)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
                if 'sovmestimost' in img_src.lower():
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏
                skip_patterns = ['logo', 'icon', 'avatar', 'banner', 'thumb', 'wp-admin', 'social']
                if any(skip in img_src.lower() for skip in skip_patterns):
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä
                width = img.get('width') or img.get('data-width') or '0'
                height = img.get('height') or img.get('data-height') or '0'
                
                try:
                    w = int(str(width).replace('px', '')) if width else 0
                    h = int(str(height).replace('px', '')) if height else 0
                    size = w * h if w > 0 and h > 0 else 1000
                    candidate_images.append((size, img_src))
                except:
                    candidate_images.append((1000, img_src))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –±–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ
        if candidate_images:
            candidate_images.sort(reverse=True, key=lambda x: x[0])
            image_url = candidate_images[0][1]
    
    return image_url

def is_fish_article(item: dict) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –æ —Ä—ã–±–µ (–∞ –Ω–µ –æ —Ä–∞—Å—Ç–µ–Ω–∏–∏ –∏–ª–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏)"""
    name = item.get('name_ru', '').lower()
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ-—Ä—ã–±—ã
    exclude_keywords = [
        '—Ä–∞—Å—Ç–µ–Ω–∏', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω', '—Ñ–∏–ª—å—Ç—Ä', '–æ–±–æ–≥—Ä–µ–≤–∞—Ç–µ–ª', '–∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä',
        '–æ—Å–≤–µ—â–µ–Ω', '–≥—Ä—É–Ω—Ç', '–¥–µ–∫–æ—Ä', '–∫–æ—Ä–º', '–ª–µ—á–µ–Ω', '–±–æ–ª–µ–∑–Ω',
        '—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö', '–∫–∞—Ç–∞–ª–æ–≥', '–æ–±–∑–æ—Ä', '—Å—Ç–∞—Ç—å', '—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç'
    ]
    
    if any(keyword in name for keyword in exclude_keywords):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä—ã–±—ã
    has_fish_params = (
        item.get('size_cm', 0) > 0 or
        item.get('min_tank_liters', 0) > 0 or
        item.get('water_params', {}).get('temp_min') is not None
    )
    
    return has_fish_params

def main():
    print("=" * 60)
    print("–ü–ï–†–ï–ü–ê–†–°–ò–ù–ì –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô –î–õ–Ø –ê–ö–í–ê–†–ò–£–ú–ù–´–• –†–´–ë")
    print("=" * 60)
    print()
    
    # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–∞—Ç–∞–ª–æ–≥
    print("üìñ –ß—Ç–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞...")
    with open(CATALOG_PATH, 'r', encoding='utf-8') as f:
        catalog_data = json.load(f)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(catalog_data)} –∑–∞–ø–∏—Å–µ–π")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ –æ —Ä—ã–±–∞—Ö
    fish_articles = [item for item in catalog_data if is_fish_article(item)]
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(fish_articles)} —Å—Ç–∞—Ç–µ–π –æ —Ä—ã–±–∞—Ö")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    articles_to_update = [
        item for item in fish_articles 
        if 'sovmestimost_akvaryb.png' in item.get('image_url', '').lower()
    ]
    print(f"üì∏ –¢—Ä–µ–±—É—é—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(articles_to_update)}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    catalog_dict = {item['id']: item for item in catalog_data}
    
    # –ü–µ—Ä–µ–ø–∞—Ä—Å–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    updated_count = 0
    not_found_count = 0
    
    print("üîÑ –ù–∞—á–∞–ª–æ –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    print()
    
    for i, item in enumerate(articles_to_update, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Å—Ç–∞—Ç—å–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π
        # –ù—É–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å URL —Å—Ç–∞—Ç—å–∏
        article_id = item.get('id')
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ URL –≤ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å
        # –û–±—ã—á–Ω–æ URL –∏–º–µ–µ—Ç —Ñ–æ—Ä–º–∞—Ç: /akvariumnye-stati/akvariumnye_rybki/{id}-{name}.html
        name_slug = item.get('name_ru', '').lower()
        name_slug = re.sub(r'[^\w\s-]', '', name_slug)
        name_slug = re.sub(r'\s+', '-', name_slug)[:50]
        
        # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ URL
        possible_urls = [
            f"https://fanfishka.ru/akvariumnye-stati/akvariumnye_rybki/{article_id}-{name_slug}.html",
            f"https://fanfishka.ru/akvariumnye-stati/akvariumnye_rybki/{article_id}.html",
        ]
        
        image_found = False
        for url in possible_urls:
            print(f"[{i}/{len(articles_to_update)}] –ü—Ä–æ–≤–µ—Ä–∫–∞: {item.get('name_ru', 'N/A')[:40]}...")
            
            soup = get_page(url)
            if soup:
                new_image = extract_image_from_page(soup, url)
                if new_image and 'sovmestimost' not in new_image.lower():
                    catalog_dict[article_id]['image_url'] = new_image
                    updated_count += 1
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {new_image[:60]}...")
                    image_found = True
                    break
            
            time.sleep(DELAY_BETWEEN_REQUESTS)
        
        if not image_found:
            not_found_count += 1
            print(f"   ‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 50 —Å—Ç–∞—Ç–µ–π
        if i % 50 == 0:
            with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
                json.dump(list(catalog_dict.values()), f, ensure_ascii=False, indent=2)
            print(f"\nüíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ({i} —Å—Ç–∞—Ç–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ)\n")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(list(catalog_dict.values()), f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 60)
    print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {updated_count}")
    print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {not_found_count}")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_PATH}")
    print()
    print("‚ú® –ì–æ—Ç–æ–≤–æ!")

if __name__ == "__main__":
    main()

